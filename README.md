ğŸš€ AI-Powered Portfolio Chatbot (RAG Backend)

Retrieval-Augmented Generation (RAG) Backend using FastAPI + LangChain + FAISS + Ollama

ğŸŒ Live Project

ğŸ”¹ Portfolio (Frontend): https://amitr2k11.github.io/

ğŸ”¹ Backend Repository: https://github.com/amitr2k11/RAG-Portfolio-Backend

The chatbot dynamically answers questions about my experience using a real RAG pipeline powered by a local LLM.

ğŸ§  Project Overview

This project implements a Retrieval-Augmented Generation (RAG) chatbot integrated into my personal portfolio website.

Instead of hardcoded responses, the chatbot:

Loads knowledge from profile.txt

Converts text into embeddings

Stores vectors using FAISS

Retrieves relevant context for each query

Generates intelligent responses using a local LLM (phi3)

âš¡ Fully local â€” no OpenAI API required.

ğŸ—ï¸ Architecture
User (GitHub Pages Frontend)
        â†“
JavaScript fetch()
        â†“
Cloudflare Tunnel (Public URL)
        â†“
FastAPI Backend (/chat)
        â†“
RAG Pipeline (LangChain + FAISS)
        â†“
Ollama (phi3 LLM)
        â†“
Generated Response

ğŸ› ï¸ Tech Stack
ğŸ”¹ Backend

FastAPI

LangChain

FAISS (Vector Store)

Ollama (Local LLM)

Uvicorn

ğŸ”¹ AI Models

phi3 â†’ Response generation

nomic-embed-text â†’ Embeddings

ğŸ”¹ Deployment

Local backend server

Exposed via Cloudflare Tunnel

Frontend hosted on GitHub Pages

ğŸ“ Project Structure
RAG-Portfolio-Backend/
â”‚â”€â”€ app.py              # FastAPI entry point
â”‚â”€â”€ rag.py              # RAG pipeline logic
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ .gitignore
â”‚
â””â”€â”€ Data/
    â””â”€â”€ profile.txt     # Knowledge base

âš™ï¸ Local Setup
1ï¸âƒ£ Install Ollama

Download from:
https://ollama.com/

Pull required models:

ollama pull phi3
ollama pull nomic-embed-text

2ï¸âƒ£ Install Python Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run Backend
uvicorn app:app --reload


Runs at:

http://127.0.0.1:8000

4ï¸âƒ£ Expose Backend (Optional - Public Access)
cloudflared tunnel --url http://127.0.0.1:8000


Copy generated URL and update frontend fetch request:

fetch("https://your-tunnel-url/chat")

ğŸ” Security

.env file excluded via .gitignore

No API keys committed

Fully local LLM (no external API dependency)

GitHub secret scanning enabled

ğŸ’¡ Why This Project Is Impressive

âœ” Real RAG implementation
âœ” Local LLM deployment
âœ” Vector search using FAISS
âœ” Frontend + Backend integration
âœ” Cloudflare tunneling setup
âœ” Secure Git workflow (secret handling, repo separation)
âœ” Production-style architecture

This demonstrates:

AI engineering fundamentals

Full-stack integration

Deployment knowledge

Secure development practices

ğŸš€ Future Improvements

Persistent backend hosting (Railway / Render / Fly.io)

Streaming token responses

Chat memory support

Admin dashboard for knowledge updates

Docker containerization

ğŸ‘¨â€ğŸ’» About Me

Amit Ranjan
Product Consultant | AI Builder | Data-Driven Problem Solver

ğŸ”— LinkedIn: https://www.linkedin.com/in/amitrnjan/

ğŸŒ Portfolio: https://amitr2k11.github.io/

â­ If You Like This Project

Give it a â­ on GitHub â€” it motivates further AI builds!
